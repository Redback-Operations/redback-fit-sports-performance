{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b3a3b2-47e0-4578-98e0-baf463510015",
   "metadata": {},
   "source": [
    "# Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566b60c-279d-40da-a38c-a6f6f0ec194d",
   "metadata": {},
   "source": [
    "#### 1. Data Loading and Preprocessing\n",
    "\n",
    "We begin by importing the curated dataset, which contains the core features—exercise metrics, sleep metrics, and the smoothed recovery time—into a pandas DataFrame. Converting the `date` column into a datetime object allows us to leverage time-based indexing and operations throughout the modeling process. To ensure efficient grouping and memory usage, the `personId` field is cast to a categorical type.\n",
    "\n",
    "Next, we arrange each user’s records in chronological order by sorting on `personId` and `date`. Setting the `date` column as the DataFrame index establishes a time series structure, which is crucial for later steps such as time-aware splitting and cross-validation. Finally, we perform basic sanity checks: reviewing the overall shape of the dataset, inspecting data types and counts, and examining summary statistics for the target variable. These checks confirm that our data is complete and correctly formatted, allowing us to proceed confidently into the modeling phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb0d51a-3a16-451d-aa8d-3d4224e73996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1250, 15)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1250 entries, 2019-11-02 to 2020-03-29\n",
      "Data columns (total 15 columns):\n",
      " #   Column                         Non-Null Count  Dtype   \n",
      "---  ------                         --------------  -----   \n",
      " 0   personId                       1250 non-null   category\n",
      " 1   exercise_activeDuration_in_ms  1250 non-null   float64 \n",
      " 2   exercise_steps                 1250 non-null   float64 \n",
      " 3   exercise_calories              1250 non-null   float64 \n",
      " 4   exercise_elevationGain         1250 non-null   float64 \n",
      " 5   exercise_averageHeartRate      1250 non-null   float64 \n",
      " 6   sleep_duration_in_ms           1250 non-null   float64 \n",
      " 7   sleep_minutesAsleep            1250 non-null   float64 \n",
      " 8   sleep_minutesAwake             1250 non-null   float64 \n",
      " 9   sleep_efficiency               1250 non-null   float64 \n",
      " 10  training_load                  1250 non-null   float64 \n",
      " 11  sleep_score                    1250 non-null   float64 \n",
      " 12  recovery_score                 1250 non-null   float64 \n",
      " 13  recovery_hours                 1250 non-null   float64 \n",
      " 14  recovery_hours_smooth          1250 non-null   float64 \n",
      "dtypes: category(1), float64(14)\n",
      "memory usage: 148.4 KB\n",
      "None\n",
      "count    1250.00\n",
      "mean       21.73\n",
      "std         5.26\n",
      "min         4.98\n",
      "25%        18.13\n",
      "50%        21.18\n",
      "75%        24.97\n",
      "max        45.10\n",
      "Name: recovery_hours_smooth, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>exercise_activeDuration_in_ms</th>\n",
       "      <th>exercise_steps</th>\n",
       "      <th>exercise_calories</th>\n",
       "      <th>exercise_elevationGain</th>\n",
       "      <th>exercise_averageHeartRate</th>\n",
       "      <th>sleep_duration_in_ms</th>\n",
       "      <th>sleep_minutesAsleep</th>\n",
       "      <th>sleep_minutesAwake</th>\n",
       "      <th>sleep_efficiency</th>\n",
       "      <th>training_load</th>\n",
       "      <th>sleep_score</th>\n",
       "      <th>recovery_score</th>\n",
       "      <th>recovery_hours</th>\n",
       "      <th>recovery_hours_smooth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-02</th>\n",
       "      <td>p01</td>\n",
       "      <td>-0.307434</td>\n",
       "      <td>-0.458858</td>\n",
       "      <td>-0.443524</td>\n",
       "      <td>-0.463559</td>\n",
       "      <td>-0.368746</td>\n",
       "      <td>0.171737</td>\n",
       "      <td>0.113733</td>\n",
       "      <td>0.556285</td>\n",
       "      <td>0.121586</td>\n",
       "      <td>-0.750958</td>\n",
       "      <td>0.235320</td>\n",
       "      <td>0.986278</td>\n",
       "      <td>18.787551</td>\n",
       "      <td>18.787551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-04</th>\n",
       "      <td>p01</td>\n",
       "      <td>-1.081904</td>\n",
       "      <td>-1.064046</td>\n",
       "      <td>-1.036455</td>\n",
       "      <td>-0.463559</td>\n",
       "      <td>-0.437021</td>\n",
       "      <td>-0.171928</td>\n",
       "      <td>-0.095485</td>\n",
       "      <td>-0.594552</td>\n",
       "      <td>-0.382128</td>\n",
       "      <td>-2.118358</td>\n",
       "      <td>-0.477613</td>\n",
       "      <td>1.640745</td>\n",
       "      <td>15.147275</td>\n",
       "      <td>16.967413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05</th>\n",
       "      <td>p01</td>\n",
       "      <td>-0.320936</td>\n",
       "      <td>0.208542</td>\n",
       "      <td>0.018905</td>\n",
       "      <td>-0.162531</td>\n",
       "      <td>1.030899</td>\n",
       "      <td>-0.582109</td>\n",
       "      <td>-0.526230</td>\n",
       "      <td>-0.758957</td>\n",
       "      <td>1.129014</td>\n",
       "      <td>-0.302031</td>\n",
       "      <td>0.602785</td>\n",
       "      <td>0.904816</td>\n",
       "      <td>19.240658</td>\n",
       "      <td>17.725161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           personId  exercise_activeDuration_in_ms  exercise_steps  \\\n",
       "date                                                                 \n",
       "2019-11-02      p01                      -0.307434       -0.458858   \n",
       "2019-11-04      p01                      -1.081904       -1.064046   \n",
       "2019-11-05      p01                      -0.320936        0.208542   \n",
       "\n",
       "            exercise_calories  exercise_elevationGain  \\\n",
       "date                                                    \n",
       "2019-11-02          -0.443524               -0.463559   \n",
       "2019-11-04          -1.036455               -0.463559   \n",
       "2019-11-05           0.018905               -0.162531   \n",
       "\n",
       "            exercise_averageHeartRate  sleep_duration_in_ms  \\\n",
       "date                                                          \n",
       "2019-11-02                  -0.368746              0.171737   \n",
       "2019-11-04                  -0.437021             -0.171928   \n",
       "2019-11-05                   1.030899             -0.582109   \n",
       "\n",
       "            sleep_minutesAsleep  sleep_minutesAwake  sleep_efficiency  \\\n",
       "date                                                                    \n",
       "2019-11-02             0.113733            0.556285          0.121586   \n",
       "2019-11-04            -0.095485           -0.594552         -0.382128   \n",
       "2019-11-05            -0.526230           -0.758957          1.129014   \n",
       "\n",
       "            training_load  sleep_score  recovery_score  recovery_hours  \\\n",
       "date                                                                     \n",
       "2019-11-02      -0.750958     0.235320        0.986278       18.787551   \n",
       "2019-11-04      -2.118358    -0.477613        1.640745       15.147275   \n",
       "2019-11-05      -0.302031     0.602785        0.904816       19.240658   \n",
       "\n",
       "            recovery_hours_smooth  \n",
       "date                               \n",
       "2019-11-02              18.787551  \n",
       "2019-11-04              16.967413  \n",
       "2019-11-05              17.725161  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Load processed data for modeling\n",
    "import pandas as pd\n",
    "\n",
    "# read in the curated file and parse the date column\n",
    "df = pd.read_csv('../data/curated/recovery_time_ready_for_model.csv', parse_dates=['date'])\n",
    "\n",
    "# convert personId to a categorical type\n",
    "df['personId'] = df['personId'].astype('category')\n",
    "\n",
    "# ensure rows are ordered by user and time, then index by date for time‐aware CV\n",
    "df.sort_values(['personId', 'date'], inplace=True)\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# quick sanity checks\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(df.info())                            \n",
    "print(df['recovery_hours_smooth'].describe().round(2))                           \n",
    "\n",
    "# preview the first few records\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5afde4-3711-411c-8a86-7b58a8fa2970",
   "metadata": {},
   "source": [
    "#### 2. Chronological Train/Validation/Test Split per User\n",
    "\n",
    "To evaluate model performance realistically, we split each user’s data chronologically into three disjoint sets:\n",
    "\n",
    "1. **Training Set (70%)**  \n",
    "   The earliest portion of each user’s timeline, used to fit model parameters.\n",
    "\n",
    "2. **Validation Set (15%)**  \n",
    "   The subsequent period, used to tune hyperparameters and select models without peeking at future data.\n",
    "\n",
    "3. **Test Set (15%)**  \n",
    "   The final segment, held out completely until the very end to assess generalization on unseen data.\n",
    "\n",
    "By grouping and slicing per `personId`, we ensure that no data from the same user overlaps across these sets, preventing information leakage. After concatenating training and validation splits, we prepare a combined **train+validation** dataset for cross-validation.  \n",
    "\n",
    "For hyperparameter tuning, we employ **GroupKFold** with five folds, using `personId` as the grouping variable. This cross-validation strategy guarantees that all records for a given user remain together in either the training or validation fold, maintaining independence between folds. Finally, we extract our feature matrix (`X`) and target vector (`y`) for both the train+validation and test sets, ready for model fitting and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12e4a1e-f6b8-4dff-ae9b-420b70d1733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:      869 rows (16 users)\n",
      "Validation set: 179 rows (15 users)\n",
      "Train+Val set:  1048 rows\n",
      "Test set:       202 rows (16 users)\n"
     ]
    }
   ],
   "source": [
    "# 2) Chronological train/val/test split per user\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def split_user(group, train_frac=0.7, val_frac=0.15):\n",
    "    n = len(group)\n",
    "    train_end = int(n * train_frac)\n",
    "    val_end = train_end + int(n * val_frac)\n",
    "    return group.iloc[:train_end], group.iloc[train_end:val_end], group.iloc[val_end:]\n",
    "\n",
    "# apply the split\n",
    "train_splits, val_splits, test_splits = [], [], []\n",
    "for user_id, user_df in df.groupby('personId', observed=True):\n",
    "    t, v, te = split_user(user_df)\n",
    "    train_splits.append(t)\n",
    "    val_splits.append(v)\n",
    "    test_splits.append(te)\n",
    "\n",
    "train_df = pd.concat(train_splits)\n",
    "val_df = pd.concat(val_splits)\n",
    "test_df = pd.concat(test_splits)\n",
    "\n",
    "# combine train+validation for cross-validation\n",
    "trainval_df = pd.concat([train_df, val_df]).sort_index()\n",
    "\n",
    "# confirm sizes\n",
    "print(f\"Train set:      {train_df.shape[0]} rows ({train_df['personId'].nunique()} users)\")\n",
    "print(f\"Validation set: {val_df.shape[0]} rows ({val_df['personId'].nunique()} users)\")\n",
    "print(f\"Train+Val set:  {trainval_df.shape[0]} rows\")\n",
    "print(f\"Test set:       {test_df.shape[0]} rows ({test_df['personId'].nunique()} users)\")\n",
    "\n",
    "# Define cross-validation strategy for hyperparameter tuning (avoiding user-based leakage)\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# define features & target\n",
    "feature_cols = [\n",
    "    'exercise_activeDuration_in_ms',\n",
    "    'exercise_steps',\n",
    "    'exercise_calories',\n",
    "    'exercise_elevationGain',\n",
    "    'exercise_averageHeartRate',\n",
    "    'sleep_duration_in_ms',\n",
    "    'sleep_minutesAsleep',\n",
    "    'sleep_minutesAwake',\n",
    "    'sleep_efficiency'\n",
    "]\n",
    "\n",
    "X_trainval = trainval_df[feature_cols]\n",
    "y_trainval = trainval_df['recovery_hours_smooth']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['recovery_hours_smooth']\n",
    "\n",
    "# Groups for GroupKFold\n",
    "groups = trainval_df['personId']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41f98f-4fce-4939-bf14-34002e5294f6",
   "metadata": {},
   "source": [
    "#### 3. Baseline Model: Constant “Mean” Predictor\n",
    "\n",
    "Before introducing complex algorithms, we establish a simple reference point using the **mean** of the training target values. This baseline model:\n",
    "\n",
    "1. **Computes the overall mean** of `recovery_hours_smooth` from the training set.\n",
    "2. **Predicts this constant mean** for every observation in both the validation and test sets.\n",
    "3. **Evaluates performance** using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).\n",
    "\n",
    "This baseline helps quantify the intrinsic difficulty of our prediction task—any model we develop should outperform this naive predictor to demonstrate real predictive value.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4542f446-5b8a-4ad6-b36e-e49dd5987088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (train mean) recovery_hours_smooth: 21.71 hours\n",
      "Validation MAE : 3.66 hours\n",
      "Validation RMSE: 4.58 hours\n",
      "Test MAE       : 3.61 hours\n",
      "Test RMSE      : 4.42 hours\n"
     ]
    }
   ],
   "source": [
    "# 3 Baseline model: constant “mean” predictor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 3.1 Compute the baseline value (mean of the training target)\n",
    "baseline_value = train_df['recovery_hours_smooth'].mean()\n",
    "print(f\"Baseline (train mean) recovery_hours_smooth: {baseline_value:.2f} hours\")\n",
    "\n",
    "# 3.2 Create constant predictions for validation and test sets\n",
    "val_preds = np.full(len(val_df), baseline_value)\n",
    "test_preds = np.full(len(test_df), baseline_value)\n",
    "\n",
    "# 3.3 Evaluate on validation set\n",
    "val_mae = mean_absolute_error(val_df['recovery_hours_smooth'], val_preds)\n",
    "val_rmse = np.sqrt(mean_squared_error(val_df['recovery_hours_smooth'], val_preds))\n",
    "print(f\"Validation MAE : {val_mae:.2f} hours\")\n",
    "print(f\"Validation RMSE: {val_rmse:.2f} hours\")\n",
    "\n",
    "# 3.4 Evaluate on test set\n",
    "test_mae = mean_absolute_error(test_df['recovery_hours_smooth'], test_preds)\n",
    "test_rmse = np.sqrt(mean_squared_error(test_df['recovery_hours_smooth'], test_preds))\n",
    "print(f\"Test MAE       : {test_mae:.2f} hours\")\n",
    "print(f\"Test RMSE      : {test_rmse:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e9b49d-3fee-44d5-89d2-f9e261612ecc",
   "metadata": {},
   "source": [
    "#### 4. Linear Regression with Time-Aware Cross-Validation\n",
    "\n",
    "In this section, we apply **Ordinary Least Squares (OLS) Linear Regression** as a straightforward, interpretable model. The main objectives are:\n",
    "\n",
    "1. **Cross-Validation (GroupKFold):**  \n",
    "   We use `GroupKFold` to perform 5-fold cross-validation on the combined train+validation set, grouping by `personId`. This ensures that each user’s data lives entirely within a single fold, preventing leakage of future information and yielding a realistic assessment of out-of-sample performance.\n",
    "\n",
    "2. **Evaluation Metrics:**  \n",
    "   We compute the **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)** across all folds to summarize predictive accuracy and robustness to larger errors.\n",
    "\n",
    "3. **Final Training and Testing:**  \n",
    "   After validating, we retrain the linear model on the full train+validation set and evaluate it once on the held-out test set, providing a final unbiased performance estimate.\n",
    "\n",
    "4. **Interpretability:**  \n",
    "   We inspect the learned coefficients for each feature to understand the **direction and relative impact** of exercise and sleep metrics on predicted recovery time.\n",
    "\n",
    "Overall, Linear Regression establishes a solid, interpretable baseline among more complex models, and its cross-validated results guide us in comparing advanced algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0704f34-f8cf-48da-a607-db2177a125f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - CV Mean MAE: 4.17 hours\n",
      "Linear Regression - CV Mean RMSE: 5.18 hours\n",
      "\n",
      "Linear Regression - Test MAE : 3.61 hours\n",
      "Linear Regression - Test RMSE: 4.42 hours\n",
      "\n",
      "Linear Regression Coefficients:\n",
      "sleep_duration_in_ms            -11.4424\n",
      "sleep_efficiency                 -0.7164\n",
      "exercise_averageHeartRate        -0.2773\n",
      "exercise_steps                   -0.1746\n",
      "exercise_activeDuration_in_ms     0.0273\n",
      "exercise_elevationGain            0.1672\n",
      "exercise_calories                 1.6816\n",
      "sleep_minutesAwake                2.2228\n",
      "sleep_minutesAsleep               9.2774\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 4 Train and evaluate Linear Regression with cross-validation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GroupKFold\n",
    "\n",
    "# 4.1 Define feature columns\n",
    "feature_cols = [\n",
    "    'exercise_activeDuration_in_ms',\n",
    "    'exercise_steps',\n",
    "    'exercise_calories',\n",
    "    'exercise_elevationGain',\n",
    "    'exercise_averageHeartRate',\n",
    "    'sleep_duration_in_ms',\n",
    "    'sleep_minutesAsleep',\n",
    "    'sleep_minutesAwake',\n",
    "    'sleep_efficiency'\n",
    "]\n",
    "\n",
    "# 4.2 Prepare train+val data for cross-validation\n",
    "X_trainval = trainval_df[feature_cols]\n",
    "y_trainval = trainval_df['recovery_hours_smooth']\n",
    "groups = trainval_df['personId']\n",
    "\n",
    "# 4.3 Cross-validation setup (GroupKFold ensures no leakage between users)\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# Evaluate using negative MAE and RMSE\n",
    "cv_mae_scores = -cross_val_score(\n",
    "    LinearRegression(),\n",
    "    X_trainval,\n",
    "    y_trainval,\n",
    "    cv=cv,\n",
    "    groups=groups,\n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "cv_rmse_scores = np.sqrt(-cross_val_score(\n",
    "    LinearRegression(),\n",
    "    X_trainval,\n",
    "    y_trainval,\n",
    "    cv=cv,\n",
    "    groups=groups,\n",
    "    scoring='neg_mean_squared_error'\n",
    "))\n",
    "\n",
    "print(f\"Linear Regression - CV Mean MAE: {cv_mae_scores.mean():.2f} hours\")\n",
    "print(f\"Linear Regression - CV Mean RMSE: {cv_rmse_scores.mean():.2f} hours\")\n",
    "\n",
    "# 4.4 Fit Linear Regression on entire trainval data and evaluate on test set\n",
    "lr_final = LinearRegression()\n",
    "lr_final.fit(X_trainval, y_trainval)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_preds = lr_final.predict(X_test)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_mae_lr = mean_absolute_error(y_test, test_preds)\n",
    "test_rmse_lr = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "\n",
    "print(f\"\\nLinear Regression - Test MAE : {test_mae:.2f} hours\")\n",
    "print(f\"Linear Regression - Test RMSE: {test_rmse:.2f} hours\")\n",
    "\n",
    "# 4.5 Inspect coefficients for interpretability\n",
    "coef_df = pd.Series(lr_final.coef_, index=feature_cols).sort_values()\n",
    "print(\"\\nLinear Regression Coefficients:\")\n",
    "print(coef_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf32c96-a200-47ac-bed0-fc3d61e60a25",
   "metadata": {},
   "source": [
    "#### 5. K-Nearest Neighbors Regression with Hyperparameter Tuning\n",
    "\n",
    "In this section, we implement a **K-Nearest Neighbors (KNN) Regressor**, which predicts a user’s recovery time as the average of the _k_ most similar historical observations. Key points:\n",
    "\n",
    "1. **KNN Principle:**  \n",
    "   Each test instance is assigned the average target value of its _k_ nearest neighbors in feature space. Smaller _k_ can capture fine-grained local patterns but may overfit; larger _k_ smooths noise but may underfit.\n",
    "\n",
    "2. **Hyperparameter Tuning (Grid Search):**  \n",
    "   We search over a range of `n_neighbors` (3, 5, 7, …, 15) to find the optimal balance between bias and variance.  \n",
    "\n",
    "3. **Time-Aware Cross-Validation (GroupKFold):**  \n",
    "   By grouping on `personId` during 5-fold cross-validation, we ensure that all records for each user reside entirely in either the training or validation fold. This prevents “peeking” at the same user’s future data when tuning _k_.\n",
    "\n",
    "4. **Final Evaluation:**  \n",
    "   After selecting the best `n_neighbors`, the tuned model is retrained on the full train+validation set and then evaluated once on the held-out test set. We report the Test MAE and RMSE to measure real-world predictive performance.\n",
    "\n",
    "This procedure systematically identifies the most appropriate neighborhood size and provides a robust estimate of KNN’s forecasting ability for recovery time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a17e50-5cb6-4c19-b2a2-5ddabb5b6ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 15\n",
      "Best CV MAE: 4.20 hours\n",
      "KNN - Test MAE : 3.47 hours\n",
      "KNN - Test RMSE: 4.26 hours\n"
     ]
    }
   ],
   "source": [
    "# 5 Train and evaluate KNN Regressor with hyperparameter tuning and cross-validation\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 5.1 Define hyperparameter grid (number of neighbors to test)\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9, 11, 13, 15]}\n",
    "\n",
    "# 5.2 Set up GroupKFold cross-validation\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# 5.3 Run GridSearchCV to find the optimal number of neighbors\n",
    "grid_knn = GridSearchCV(\n",
    "    estimator=KNeighborsRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform Grid Search on training + validation set\n",
    "grid_knn.fit(X_trainval, y_trainval, groups=groups)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(f\"Best n_neighbors: {grid_knn.best_params_['n_neighbors']}\")\n",
    "print(f\"Best CV MAE: {-grid_knn.best_score_:.2f} hours\")\n",
    "\n",
    "# 5.4 Evaluate the best model on the test set\n",
    "best_knn = grid_knn.best_estimator_\n",
    "test_preds_knn = best_knn.predict(X_test)\n",
    "\n",
    "test_mae_knn = mean_absolute_error(y_test, test_preds_knn)\n",
    "test_rmse_knn = np.sqrt(mean_squared_error(y_test, test_preds_knn))\n",
    "\n",
    "print(f\"KNN - Test MAE : {test_mae_knn:.2f} hours\")\n",
    "print(f\"KNN - Test RMSE: {test_rmse_knn:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b2d1c-d8f8-47c8-81b4-ae37a6fc28f6",
   "metadata": {},
   "source": [
    "#### 6. Decision Tree Regression with Hyperparameter Tuning\n",
    "\n",
    "Here, we apply a **Decision Tree Regressor**, a non-parametric model that recursively splits the feature space into homogeneous regions. Key aspects:\n",
    "\n",
    "1. **Decision Tree Mechanics:**  \n",
    "   The tree partitions data by choosing feature thresholds that minimize error (e.g., MAE) at each split. Leaves then predict the average target value of their region.\n",
    "\n",
    "2. **Regularization via Hyperparameters:**  \n",
    "   - `max_depth`: Limits tree height to prevent overfitting on noise (shallow trees generalize better).  \n",
    "   - `min_samples_split`: Minimum number of samples required to split an internal node.  \n",
    "   - `min_samples_leaf`: Minimum number of samples required to be at a leaf node, ensuring each prediction is based on enough data.\n",
    "\n",
    "3. **Grid Search with GroupKFold:**  \n",
    "   We systematically search over combinations of these hyperparameters using 5-fold **GroupKFold** cross-validation, grouping by `personId`. This ensures each user’s data remains intact within each fold, avoiding leakage.\n",
    "\n",
    "4. **Final Evaluation:**  \n",
    "   The tuned tree is retrained on the full train+validation set using the best hyperparameters, then tested on the held-out test set. We report the Test MAE and Test RMSE to assess generalization performance.\n",
    "\n",
    "Decision trees provide easily interpretable decision rules, but require careful tuning to balance bias and variance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e59ae94-9b47-4e53-b188-f65fbfd55cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best CV MAE: 4.21 hours\n",
      "Decision Tree - Test MAE : 3.50 hours\n",
      "Decision Tree - Test RMSE: 4.39 hours\n"
     ]
    }
   ],
   "source": [
    "# 6 Train and evaluate Decision Tree Regressor with hyperparameter tuning and cross-validation\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 6.1 Define hyperparameter grid to tune (common Decision Tree hyperparameters)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, None],  # maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # minimum samples required to split\n",
    "    'min_samples_leaf': [1, 2, 4]     # minimum samples required at each leaf node\n",
    "}\n",
    "\n",
    "# 6.2 Setup GroupKFold cross-validation\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# 6.3 Perform GridSearchCV to find optimal hyperparameters\n",
    "grid_dt = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on training + validation data\n",
    "grid_dt.fit(X_trainval, y_trainval, groups=groups)\n",
    "\n",
    "# Display the best hyperparameters found\n",
    "print(f\"Best Hyperparameters: {grid_dt.best_params_}\")\n",
    "print(f\"Best CV MAE: {-grid_dt.best_score_:.2f} hours\")\n",
    "\n",
    "# 6.4 Evaluate best model on test set\n",
    "best_dt = grid_dt.best_estimator_\n",
    "test_preds_dt = best_dt.predict(X_test)\n",
    "\n",
    "test_mae_dt = mean_absolute_error(y_test, test_preds_dt)\n",
    "test_rmse_dt = np.sqrt(mean_squared_error(y_test, test_preds_dt))\n",
    "\n",
    "print(f\"Decision Tree - Test MAE : {test_mae_dt:.2f} hours\")\n",
    "print(f\"Decision Tree - Test RMSE: {test_rmse_dt:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a86279-324a-4909-8bde-c7d2f1a97b3f",
   "metadata": {},
   "source": [
    "#### 7. Random Forest Regression with Hyperparameter Tuning\n",
    "\n",
    "In this section, we leverage a **Random Forest Regressor**, an ensemble of decision trees that reduces variance and improves generalization. Key points:\n",
    "\n",
    "1. **Random Forest Principles:**  \n",
    "   - Builds multiple decision trees on bootstrap samples of the data.  \n",
    "   - At each split, a random subset of features is considered, which decorrelates trees and prevents overfitting.\n",
    "\n",
    "2. **Hyperparameter Tuning:**  \n",
    "   - `n_estimators`: Number of trees in the forest — more trees can improve stability at the cost of computation.  \n",
    "   - `max_depth`, `min_samples_split`, `min_samples_leaf`: Control tree complexity and leaf sizes to balance bias and variance.\n",
    "\n",
    "3. **Grid Search with GroupKFold:**  \n",
    "   We perform a systematic grid search across these hyperparameters using 5-fold **GroupKFold**, grouping by `personId`. This ensures that each user’s observations remain entirely in the training or validation fold, preventing leakage of future information.\n",
    "\n",
    "4. **Final Model Evaluation:**  \n",
    "   After selecting the best hyperparameters, the Random Forest is retrained on the combined train+validation set and evaluated on the held-out test set. We report Test MAE and RMSE to quantify its predictive performance.\n",
    "\n",
    "Random Forests typically deliver robust, high-accuracy predictions and can handle complex non-linear relationships, making them a strong candidate for our recovery time estimator.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55eeb09-8fee-4afe-830a-ba71964f0959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 150}\n",
      "Best CV MAE: 4.11 hours\n",
      "Random Forest - Test MAE : 3.38 hours\n",
      "Random Forest - Test RMSE: 4.20 hours\n"
     ]
    }
   ],
   "source": [
    "# 7 Train and evaluate Random Forest Regressor with hyperparameter tuning and cross-validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 7.1 Define hyperparameter grid to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 7.2 Setup GroupKFold cross-validation\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# 7.3 Perform GridSearchCV to identify optimal hyperparameters\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on train+val data\n",
    "grid_rf.fit(X_trainval, y_trainval, groups=groups)\n",
    "\n",
    "# Display best hyperparameters found\n",
    "print(f\"Best Hyperparameters: {grid_rf.best_params_}\")\n",
    "print(f\"Best CV MAE: {-grid_rf.best_score_:.2f} hours\")\n",
    "\n",
    "# 7.4 Evaluate best model on test set\n",
    "best_rf = grid_rf.best_estimator_\n",
    "test_preds_rf = best_rf.predict(X_test)\n",
    "\n",
    "test_mae_rf = mean_absolute_error(y_test, test_preds_rf)\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, test_preds_rf))\n",
    "\n",
    "print(f\"Random Forest - Test MAE : {test_mae_rf:.2f} hours\")\n",
    "print(f\"Random Forest - Test RMSE: {test_rmse_rf:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51a76a-475f-469b-81cd-b15842bad14e",
   "metadata": {},
   "source": [
    "#### 8. Gradient Boosting Regression with Hyperparameter Tuning\n",
    "\n",
    "Gradient Boosting builds an ensemble model by sequentially fitting new trees to the **residual errors** of the combined previous learners. This approach often yields high predictive accuracy by minimizing bias and variance through additive corrections. Key points:\n",
    "\n",
    "1. **Boosting Concept:**  \n",
    "   Each new tree focuses on the samples that the existing ensemble predicts poorly, iteratively reducing overall residual error.\n",
    "\n",
    "2. **Hyperparameter Tuning:**  \n",
    "   - `n_estimators`: Number of boosting stages (trees) — more stages can improve fit but risk overfitting.  \n",
    "   - `learning_rate`: Shrinks the contribution of each tree, balancing the trade-off between model complexity and convergence speed.  \n",
    "   - `max_depth`, `min_samples_split`, `min_samples_leaf`: Control individual tree complexity and leaf sizes to prevent overfitting.\n",
    "\n",
    "3. **Group-K-Fold Cross-Validation:**  \n",
    "   We search this hyperparameter grid via 5-fold **GroupKFold**, grouping by `personId`. This keeps each user’s data intact within folds, ensuring unbiased performance estimates.\n",
    "\n",
    "4. **Final Evaluation:**  \n",
    "   After identifying optimal hyperparameters, the model is retrained on the full train+validation set and evaluated once on the hold-out test set. We report Test MAE and RMSE to quantify real-world predictive performance.\n",
    "\n",
    "Gradient Boosting often outperforms single-tree methods by combining many weak learners into a strong ensemble, making it a powerful choice for our recovery time prediction task.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8e19cd-a70e-44bf-a437-22331a74e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 150}\n",
      "Best CV MAE: 4.09 hours\n",
      "Gradient Boosting - Test MAE : 3.37 hours\n",
      "Gradient Boosting - Test RMSE: 4.13 hours\n"
     ]
    }
   ],
   "source": [
    "# 8 Train and evaluate Gradient Boosting Regressor with hyperparameter tuning and cross-validation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 8.1 Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 8.2 Setup GroupKFold cross-validation\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# 8.3 Perform GridSearchCV\n",
    "grid_gbr = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on train+val data\n",
    "grid_gbr.fit(X_trainval, y_trainval, groups=groups)\n",
    "\n",
    "# Display best hyperparameters found\n",
    "print(f\"Best Hyperparameters: {grid_gbr.best_params_}\")\n",
    "print(f\"Best CV MAE: {-grid_gbr.best_score_:.2f} hours\")\n",
    "\n",
    "# 8.4 Evaluate best model on test set\n",
    "best_gbr = grid_gbr.best_estimator_\n",
    "test_preds_gbr = best_gbr.predict(X_test)\n",
    "\n",
    "test_mae_gbr = mean_absolute_error(y_test, test_preds_gbr)\n",
    "test_rmse_gbr = np.sqrt(mean_squared_error(y_test, test_preds_gbr))\n",
    "\n",
    "print(f\"Gradient Boosting - Test MAE : {test_mae_gbr:.2f} hours\")\n",
    "print(f\"Gradient Boosting - Test RMSE: {test_rmse_gbr:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5cec6-4b85-4997-b210-54c463889719",
   "metadata": {},
   "source": [
    "#### 9. Support Vector Regression with Hyperparameter Tuning\n",
    "\n",
    "Support Vector Regression (SVR) extends the principles of Support Vector Machines (SVM) to regression tasks by finding a function that deviates from the true targets by no more than a specified margin (`epsilon`) while remaining as flat as possible. Key elements:\n",
    "\n",
    "1. **SVR Fundamentals:**  \n",
    "   - Seeks a regression function within an “epsilon-insensitive tube,” ignoring errors smaller than `epsilon`.  \n",
    "   - Uses kernel functions (here, RBF) to model complex, non-linear relationships by projecting data into a higher-dimensional feature space.\n",
    "\n",
    "2. **Hyperparameters:**  \n",
    "   - `C`: Regularization parameter controlling the trade-off between model flatness and tolerance for deviations larger than `epsilon`. Higher `C` permits less deviation but risks overfitting.  \n",
    "   - `epsilon`: Width of the no-penalty tube around the regression function. Larger `epsilon` yields a simpler, more generalized model.  \n",
    "   - `kernel`: Selected as `'rbf'` to capture non-linear patterns.\n",
    "\n",
    "3. **GroupKFold Cross-Validation:**  \n",
    "   We tune `C` and `epsilon` via 5-fold **GroupKFold**, grouping by `personId`. This ensures that all observations for each user remain together in either the training or validation fold, preventing data leakage.\n",
    "\n",
    "4. **Final Model Assessment:**  \n",
    "   After finding the optimal hyperparameters, the SVR model is retrained on the combined train+validation set and evaluated on the held-out test set. We report the Test MAE and RMSE to quantify its predictive accuracy.\n",
    "\n",
    "SVR is particularly effective for medium-sized datasets with non-linear relationships, offering robust generalization through margin-based optimization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225fa37d-50c2-421d-a762-34fe7d467621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'epsilon': 1, 'kernel': 'rbf'}\n",
      "Best CV MAE: 4.14 hours\n",
      "SVR - Test MAE : 3.36 hours\n",
      "SVR - Test RMSE: 4.19 hours\n"
     ]
    }
   ],
   "source": [
    "# 9 Train and evaluate SVR with hyperparameter tuning and cross-validation\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# 9.1 Define hyperparameter grid for SVR tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# 9.2 Setup GroupKFold cross-validation\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# 9.3 Perform GridSearchCV\n",
    "grid_svr = GridSearchCV(\n",
    "    estimator=SVR(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on train+val data\n",
    "grid_svr.fit(X_trainval, y_trainval, groups=groups)\n",
    "\n",
    "# Display best hyperparameters found\n",
    "print(f\"Best Hyperparameters: {grid_svr.best_params_}\")\n",
    "print(f\"Best CV MAE: {-grid_svr.best_score_:.2f} hours\")\n",
    "\n",
    "# 9.4 Evaluate best SVR model on test set\n",
    "best_svr = grid_svr.best_estimator_\n",
    "test_preds_svr = best_svr.predict(X_test)\n",
    "\n",
    "test_mae_svr = mean_absolute_error(y_test, test_preds_svr)\n",
    "test_rmse_svr = np.sqrt(mean_squared_error(y_test, test_preds_svr))\n",
    "\n",
    "print(f\"SVR - Test MAE : {test_mae_svr:.2f} hours\")\n",
    "print(f\"SVR - Test RMSE: {test_rmse_svr:.2f} hours\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798a0e3-25ef-44f4-8782-ca76a1d21a5b",
   "metadata": {},
   "source": [
    "#### 10. Model Comparison and Selection\n",
    "\n",
    "After training and tuning all candidate models, we compile their performance metrics into a unified table for direct comparison. This summary includes:\n",
    "\n",
    "- **Cross-Validation MAE** (where applicable) to assess each model’s average performance during hyperparameter tuning on the train+validation data.\n",
    "- **Test MAE and RMSE** to evaluate real-world accuracy and error dispersion on the held-out test set.\n",
    "\n",
    "By presenting these metrics side by side, we can easily identify which algorithm achieves the lowest prediction error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7883acf-6a74-436e-aaa1-ff6c44d894a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison Summary:\n",
      "               Model  CV MAE  Test MAE  Test RMSE\n",
      "0           Baseline     NaN      3.61       4.42\n",
      "1  Linear Regression    4.17      3.58       4.50\n",
      "2                KNN    4.20      3.47       4.26\n",
      "3      Decision Tree    4.21      3.50       4.39\n",
      "4      Random Forest    4.11      3.38       4.20\n",
      "5  Gradient Boosting    4.09      3.37       4.13\n",
      "6                SVR    4.14      3.36       4.19\n"
     ]
    }
   ],
   "source": [
    "# 10 Summarize all model performances clearly\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Baseline',\n",
    "        'Linear Regression',\n",
    "        'KNN',\n",
    "        'Decision Tree',\n",
    "        'Random Forest',\n",
    "        'Gradient Boosting',\n",
    "        'SVR'\n",
    "    ],\n",
    "    'CV MAE': [\n",
    "        np.nan,  # Baseline (no CV)\n",
    "        4.17,\n",
    "        4.20,\n",
    "        4.21,\n",
    "        4.11,\n",
    "        4.09,\n",
    "        grid_svr.best_score_ * -1\n",
    "    ],\n",
    "    'Test MAE': [\n",
    "        test_mae,\n",
    "        test_mae_lr,\n",
    "        test_mae_knn,\n",
    "        test_mae_dt,\n",
    "        test_mae_rf,\n",
    "        test_mae_gbr,\n",
    "        test_mae_svr\n",
    "    ],\n",
    "    'Test RMSE': [\n",
    "        test_rmse,\n",
    "        test_rmse_lr,\n",
    "        test_rmse_knn,\n",
    "        test_rmse_dt,\n",
    "        test_rmse_rf,\n",
    "        test_rmse_gbr,\n",
    "        test_rmse_svr\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Round metrics for readability\n",
    "results[['CV MAE', 'Test MAE', 'Test RMSE']] = results[['CV MAE', 'Test MAE', 'Test RMSE']].round(2)\n",
    "\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d642a1-7f39-47e5-9f46-a891500b8432",
   "metadata": {},
   "source": [
    "#### 11. Final Model Selection\n",
    "\n",
    "With all models evaluated on the same test set, we programmatically:\n",
    "\n",
    "1. **Identify the Best Model** by locating the lowest Test MAE in our comparison table.  \n",
    "2. **Print the Selection** to confirm which algorithm and hyperparameter configuration achieved optimal accuracy.  \n",
    "3. **Map Model Names to Objects** so we can reference and persist the chosen estimator.  \n",
    "4. **Retrieve the Best Model Instance** from our dictionary of tuned models for saving or deployment.\n",
    "\n",
    "This step completes the model development pipeline by finalizing our choice of algorithm based on objective performance metrics.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40860d01-c7f6-4b91-aa7f-aded63186747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best model based on Test MAE: SVR (Test MAE = 3.36 hours)\n"
     ]
    }
   ],
   "source": [
    "# 11 Select the best model based on Test MAE\n",
    "best_idx = results['Test MAE'].idxmin()\n",
    "best_model_name = results.loc[best_idx, 'Model']\n",
    "best_test_mae = results.loc[best_idx, 'Test MAE']\n",
    "\n",
    "print(f\"\\n Best model based on Test MAE: {best_model_name} (Test MAE = {best_test_mae:.2f} hours)\")\n",
    "\n",
    "# Map the final (hyperparameter-tuned) fitted models\n",
    "model_map = {\n",
    "    'Linear Regression': lr_final,\n",
    "    'KNN': best_knn,\n",
    "    'Decision Tree': best_dt,\n",
    "    'Random Forest': best_rf,\n",
    "    'Gradient Boosting': best_gbr,\n",
    "    'SVR': best_svr\n",
    "}\n",
    "\n",
    "# Retrieve the best model instance\n",
    "best_model = model_map[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a935a4-8c0f-470f-9d74-a28669a7bec2",
   "metadata": {},
   "source": [
    "#### 12. Persisting the Selected Model\n",
    "\n",
    "To enable future use—whether for batch inference, real-time serving, or further analysis—we serialize the best-performing model to disk using `joblib`. This approach:\n",
    "\n",
    "1. **Exports the Model Object** into a `.pkl` file, capturing all learned parameters and hyperparameters.  \n",
    "2. **Ensures Reproducibility**, since the exact same model can be loaded later without retraining.  \n",
    "3. **Facilitates Deployment**, allowing downstream applications or scripts to quickly load the model and generate predictions on new data.\n",
    "\n",
    "With this final step, the entire modeling pipeline is complete, from data ingestion and preprocessing through model training, evaluation, selection, and persistence.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d13995-8ea1-456a-8346-c36dcff6c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SVR model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 12 Save the best model to workspace using joblib\n",
    "joblib.dump(best_model, f\"{best_model_name.replace(' ', '_').lower()}_model.pkl\")\n",
    "print(f\"\\n {best_model_name} model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
